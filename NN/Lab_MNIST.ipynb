{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Лабораторная №2\n",
    "\n",
    "## Распознавание MNIST на CNTK\n",
    "\n",
    "Посмотрим, насколько хорошо у вас получится распознать с помощью CNTK уже известные нам рукописные цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cntk\n",
    "from cntk import Trainer\n",
    "from cntk.learners import sgd\n",
    "from cntk.ops import *\n",
    "from cntk.io import *\n",
    "from cntk.layers import *\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('mnist.pkl'):\n",
    "    !rm *.pkl\n",
    "    !wget https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/MNIST/mnist.pkl.gz\n",
    "    !gzip -d mnist.pkl.gz\n",
    "        \n",
    "with open('mnist.pkl', 'rb') as mnist_pickle:\n",
    "    MNIST = pickle.load(mnist_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили объект `MNIST`, содержащий все необходимые данные. Проведем с ними небольшие преобразования:\n",
    "\n",
    "  * Нормируем, чтобы яркость пискселей была в диапазоне $[0,1]$\n",
    "  * Приведём labels к векторному виду с помощью функции $\\mathrm{conv}: \\mathbb{N} \\to \\mathbb{N}^{10}$, где $i\\mapsto \\left(\\mathrm{eq}(i,j)\\right)_{j=0}^9$, и $\\mathrm{eq}(i,j) = \\begin{cases}\n",
    "         1, & i=j \\\\\n",
    "         0, & i\\not=j\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = MNIST['Train']['Features'].astype(np.float32) / 256.0\n",
    "labels = MNIST['Train']['Labels']\n",
    "\n",
    "def conv(n):\n",
    "    return np.eye(10,dtype=np.float32)[n]\n",
    "\n",
    "labels = conv(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описываем переменные, модель и ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = input_variable(784)\n",
    "label_var = input_variable(10)\n",
    "\n",
    "model = Sequential([\n",
    "          Dense(2000, init=glorot_uniform(),activation=relu),\n",
    "          Dense(10, init=glorot_uniform(),activation=None)]);\n",
    "\n",
    "# Подсказка: попробуйте использовать model = Sequential([....]) для комбинации слоёв!\n",
    "\n",
    "z = model(input_var)\n",
    "\n",
    "ce = cntk.cross_entropy_with_softmax(z, label_var)\n",
    "pe = cntk.classification_error(z, label_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описываем `learner` и `trainer`. Переменная `progress` будет хранить достигнутую точность на разных шагах обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1590010 parameters in 4 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 16\n",
    "\n",
    "lr_per_minibatch = cntk.learning_rate_schedule(0.1, cntk.UnitType.minibatch)\n",
    "pp = cntk.logging.ProgressPrinter()\n",
    "    \n",
    "learner = cntk.adagrad(z.parameters, lr = lr_per_minibatch)\n",
    "trainer = cntk.Trainer(z, (ce, pe), [learner],[pp])\n",
    "\n",
    "cntk.logging.log_number_of_parameters(z)\n",
    "progress = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем обучение. Обратите внимание, что вы можете менять количество эпох и минибатчей в каждой эпохе. Можете запускать этот блок несколько раз, он будет дообучать модель и достраивать график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.1\n",
      "Finished Epoch[1]: loss = 0.532714 * 33600, metric = 13.29% * 33600 29.422s (1142.0 samples/s);\n",
      "Finished Epoch[2]: loss = 0.272278 * 33600, metric = 7.60% * 33600 26.499s (1268.0 samples/s);\n",
      "Finished Epoch[3]: loss = 0.233809 * 33600, metric = 6.54% * 33600 25.159s (1335.5 samples/s);\n",
      "Finished Epoch[4]: loss = 0.210023 * 33600, metric = 5.83% * 33600 25.295s (1328.3 samples/s);\n",
      "Finished Epoch[5]: loss = 0.193452 * 33600, metric = 5.44% * 33600 24.967s (1345.8 samples/s);\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    perm = np.random.permutation(len(labels_train))\n",
    "    taccuracy = 0; tloss = 0; cnt = 0\n",
    "    for i in range(0, len(labels_train), minibatch_size):\n",
    "        max_n = min(i + minibatch_size, len(labels_train))\n",
    "        x = features_train[perm[i:max_n]]\n",
    "        t = labels_train[perm[i:max_n]]\n",
    "        trainer.train_minibatch({input_var:x,label_var:t})\n",
    "        tloss += trainer.previous_minibatch_loss_average*trainer.previous_minibatch_sample_count\n",
    "        taccuracy += trainer.previous_minibatch_evaluation_average*trainer.previous_minibatch_sample_count\n",
    "        cnt+=trainer.previous_minibatch_sample_count\n",
    "        pp.update_with_trainer(trainer,with_metric=True)\n",
    "    progress.append([tloss,taccuracy])\n",
    "    pp.epoch_summary(with_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем вычислить функцию потерь и ошибку на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.21345090866088867, Error=0.05964285880327225\n"
     ]
    }
   ],
   "source": [
    "vloss = np.average(ce.eval({input_var:features_test,label_var:labels_test}))\n",
    "verr = np.average(pe.eval({input_var:features_test,label_var:labels_test}))\n",
    "print(\"Loss={}, Error={}\".format(vloss,verr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x):\n",
    "    fig = pylab.figure()\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    pylab.imshow(x.reshape(28,28))\n",
    "    p = cntk.softmax(z)\n",
    "    hist = p.eval({input_var:x})\n",
    "    print(hist[0][0])\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    pylab.bar(np.arange(10),hist[0][0])\n",
    "    pylab.xticks(np.arange(10))\n",
    "    pylab.show()\n",
    "    \n",
    "plot(features_test[0])\n",
    "plot(features_test[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "1. Попробовать достичь минимальной величины ошибки на тестовых данных за счет изменения архитектуры сети\n",
    "   - попробуйте многослойный персептрон\n",
    "   - попробуйте разные функции активации\n",
    "2. В процессе обучения строить training error и testing error и посмотреть, если ли переобучение\n",
    "3. Попробовать обучение на виртуальных машинах с GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Лабораторная №3\n",
    "\n",
    "## Улучшаем качество MNIST с помощью свёрточной сети\n",
    "\n",
    "Давайте попробуем максимально увеличить качество распознавания, используя свёрточную архитектуру. Можно начать с примера ниже, и затем поэкспериментировать:\n",
    "\n",
    "  * с количеством свёрточных слоёв\n",
    "  * с размером окна свёртки (3x3, 5x5, 7x7)\n",
    "  * с количеством выходных слоёв классификатора\n",
    " \n",
    "Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Convolution((5,5), 32, init=glorot_uniform(), pad=True,activation=relu),\n",
    "        MaxPooling((3,3), strides=(2,2)),\n",
    "        Dense(64, init=glorot_uniform(),activation=relu),\n",
    "        Dense(10, init=glorot_uniform(), activation=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 347658 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "input_var = input_variable((1,28,28))\n",
    "label_var = input_variable(10)\n",
    "\n",
    "z = model(input_var)\n",
    "\n",
    "ce = cntk.cross_entropy_with_softmax(z, label_var)\n",
    "pe = cntk.classification_error(z, label_var)\n",
    "\n",
    "minibatch_size = 16\n",
    "\n",
    "lr_per_minibatch = cntk.learning_rate_schedule(0.1, cntk.UnitType.minibatch)\n",
    "pp = cntk.logging.ProgressPrinter()\n",
    "    \n",
    "learner = cntk.adagrad(z.parameters, lr = lr_per_minibatch)\n",
    "trainer = cntk.Trainer(z, (ce, pe), [learner],[pp])\n",
    "\n",
    "cntk.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[1]: loss = 1.551183 * 51728, metric = 42.47% * 51728 302.034s (171.3 samples/s);\n",
      "Finished Epoch[2]: loss = 1.197694 * 33600, metric = 33.47% * 33600 132.901s (252.8 samples/s);\n",
      "Finished Epoch[3]: loss = 1.104741 * 33600, metric = 31.46% * 33600 131.291s (255.9 samples/s);\n",
      "Finished Epoch[4]: loss = 1.047783 * 33600, metric = 30.24% * 33600 129.562s (259.3 samples/s);\n",
      "Finished Epoch[5]: loss = 1.008365 * 33600, metric = 29.36% * 33600 112.571s (298.5 samples/s);\n"
     ]
    }
   ],
   "source": [
    "feat_tr = features_train.reshape(-1,1,28,28)\n",
    "for epoch in range(5):\n",
    "    perm = np.random.permutation(len(labels_train))\n",
    "    taccuracy = 0; tloss = 0; cnt = 0\n",
    "    for i in range(0, len(labels_train), minibatch_size):\n",
    "        max_n = min(i + minibatch_size, len(labels_train))\n",
    "        x = feat_tr[perm[i:max_n]]\n",
    "        t = labels_train[perm[i:max_n]]\n",
    "        trainer.train_minibatch({input_var:x,label_var:t})\n",
    "        tloss += trainer.previous_minibatch_loss_average*trainer.previous_minibatch_sample_count\n",
    "        taccuracy += trainer.previous_minibatch_evaluation_average*trainer.previous_minibatch_sample_count\n",
    "        cnt+=trainer.previous_minibatch_sample_count\n",
    "        pp.update_with_trainer(trainer,with_metric=True)\n",
    "    progress.append([tloss,taccuracy])\n",
    "    pp.epoch_summary(with_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
