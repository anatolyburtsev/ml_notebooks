{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "df_majority = data[data.Label==0]\n",
    "df_minority = data[data.Label==1]\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=df_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "# df_majority_downsampled = resample(df_majority,\n",
    "#                                   replace=True,\n",
    "#                                   n_samples=df_minority.shape[0],\n",
    "#                                   random_state=123)\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "size = 3000\n",
    "a = resample(df_minority,\n",
    "             replace=True,\n",
    "             n_samples=size,\n",
    "             random_state=123)\n",
    "b = resample(df_majority,\n",
    "            replace=True,\n",
    "            n_samples=size,\n",
    "            random_state=123)\n",
    "# data = pd.concat([df_minority, df_majority_downsampled])\n",
    "data = pd.concat([a,b])\n",
    "data = data.sample(frac=1)\n",
    "data = data.reset_index(drop=False).drop([\"index\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Word\"] = data[\"Word\"].apply(lambda x: x.lower())\n",
    "\n",
    "vowel = u'уеёыаоэяию'\n",
    "consonant = u'йцкнгшщзхъфвпрлджчсмтьб'\n",
    "ru_letter = vowel + consonant + u\"-’'\"\n",
    "last_letter = list(set(data[\"Word\"].apply(lambda x: x[-1]).get_values()))\n",
    "last_2_letter = list(set(data[\"Word\"].apply(lambda x: x[-2:]).get_values()))\n",
    "last_3_letter = list(set(data[\"Word\"].apply(lambda x: x[-3:]).get_values()))\n",
    "\n",
    "def count_vow(word):\n",
    "    vow = 0\n",
    "    for i in word:\n",
    "        if i in vowel:\n",
    "            vow += 1\n",
    "    return vow\n",
    "\n",
    "def count_con(word):\n",
    "    cons = 0\n",
    "    for i in word:\n",
    "        if i in consonant:\n",
    "            cons += 1\n",
    "    return cons\n",
    "\n",
    "def is_only_ru(word):\n",
    "    return len(set(word) - set(ru_letter)) == 0\n",
    "\n",
    "\n",
    "def last_letters(data):\n",
    "    for letter in  last_letter:\n",
    "        data[\"last1\" + letter] = data[\"Word\"].apply(lambda x: x[-1] == letter)\n",
    "    for letter in last_2_letter:\n",
    "        data[\"last2\" + letter] = data[\"Word\"].apply(lambda x: x[-2:] == letter)\n",
    "#     for letter in last_3_letter:\n",
    "#         data[\"last2\" + letter] = data[\"Word\"].apply(lambda x: x[-3:] == letter)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_onotoles_old_features(data):\n",
    "    data = data[data[\"Word\"].apply(lambda x: is_only_ru(x))==1]\n",
    "    data[\"len\"] = data[\"Word\"].apply(lambda x: len(x))\n",
    "    data[\"is_len_more_11\"] = data[\"len\"].apply(lambda x: x > 11)\n",
    "    data[\"vow_count\"] = data[\"Word\"].apply(lambda x: count_vow(x))\n",
    "    data[\"cons_count\"] = data[\"Word\"].apply(lambda x: count_con(x))\n",
    "    data[\"last_vow\"] = data[\"Word\"].apply(lambda x: x[-1] in vowel)\n",
    "    data[\"prelast_vow\"] = data[\"Word\"].apply(lambda x: len(x) > 2 and x[-2] in vowel)\n",
    "    data[\"ohara\"] = data[\"Word\"].apply(lambda x: x[:2] == \"о'\")\n",
    "    data = last_letters(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786957779626173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = data[\"Word\"]\n",
    "y = data[\"Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2,7))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "#                      ('clf', xgb.XGBClassifier(colsample_bytree=0.97, max_depth=10, n_estimators=90, subsample=0.97))\n",
    "                     ('clf', SGDClassifier(alpha=1e-4, penalty='l2'))#alpha=1e-4, penalty='l2', class_weight='balanced')),\n",
    "                    ])\n",
    "# print( cross_val_score(text_clf, X=X, y=y, scoring='roc_auc').mean())\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "print(roc_auc_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7908117305575018\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer='char_wb', ngram_range=(2,7))\n",
    "tfidf = TfidfTransformer(use_idf=True)\n",
    "clf = LogisticRegression(C=2, class_weight='balanced')\n",
    "\n",
    "count_transformer = vect.fit(X_train)\n",
    "counts = count_transformer.transform(X_train)\n",
    "tfidf_transformer = tfidf.fit(counts)\n",
    "data_tfidf = tfidf_transformer.transform(counts)\n",
    "clf.fit(data_tfidf, y_train)\n",
    "\n",
    "c_2 = count_transformer.transform(X_test)\n",
    "tfidf_2 = tfidf_transformer.transform(c_2)\n",
    "\n",
    "y_pred = clf.predict(tfidf_2)\n",
    "print(roc_auc_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71650913, 0.28349087],\n",
       "       [0.50270216, 0.49729784],\n",
       "       [0.11621142, 0.88378858],\n",
       "       ...,\n",
       "       [0.83080795, 0.16919205],\n",
       "       [0.23931248, 0.76068752],\n",
       "       [0.27013066, 0.72986934]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict_proba(tfidf_2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(X, y)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test = test[\"Word\"].apply(lambda x: x.lower())\n",
    "pred = text_clf.predict(test)\n",
    "pd.Series(pred).to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 83.8min\n"
     ]
    }
   ],
   "source": [
    "# {'vect__ngram_range': (2, 7), 'clf__alpha': 1e-05}\n",
    "# 0.896830120806\n",
    "# parameters = {'vect__ngram_range': [(2,7)],\n",
    "# #               'tfidf__use_idf': True,\n",
    "#               'clf__alpha': [1e-4],\n",
    "#               'clf__penalty': ['l2', 'l1'],\n",
    "#               'clf__class_weight': ['balanced']\n",
    "#              }\n",
    "parameters = {\n",
    "    'clf__n_jobs': [-1],\n",
    "    'clf__max_depth': [22, 25, 30, 40, 50, 70, 100, 120, 130],\n",
    "    'clf__n_estimators' : [40, 50, 60, 100, 200]\n",
    "#     'clf__max_features' : ['log2', None, 'sqrt']\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, verbose=1, scoring='roc_auc')\n",
    "gs_clf = gs_clf.fit(X, y)\n",
    "print(gs_clf.best_params_)\n",
    "print(gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_features(data)\n",
    "X = data.drop([\"Label\", \"Word\"], axis=1)\n",
    "y = data[\"Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "regr = xgb.XGBClassifier(colsample_bytree=0.97, max_depth=10, n_estimators=90, subsample=0.97)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "print(roc_auc_score(y_pred, y_test.get_values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier = xgb.XGBClassifier()\n",
    "# # {'max_depth': 11, 'subsample': 0.9}\n",
    "# # {'colsample_bytree': 0.95, 'max_depth': 10, 'n_estimators': 80, 'subsample': 0.97}\n",
    "# # 0.7825313337289386\n",
    "# params = {\n",
    "#     'max_depth': [9, 10, 11],\n",
    "#     'subsample': [0.97, 0.99, 1],\n",
    "#     'n_estimators': [70, 80, 90],\n",
    "#     'colsample_bytree': [0.95, 0.97, 1]\n",
    "# }\n",
    "# grid_search = GridSearchCV(classifier, params, n_jobs=-1, verbose=1, scoring='roc_auc')\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier()\n",
    "# params = {\n",
    "#     'max_depth': [22, 25, 30, 40, 50, 70, 100],\n",
    "#     'n_estimators' : [40, 50, 60, 100],\n",
    "#     'max_features' : ['log2', None, 'sqrt']\n",
    "# }\n",
    "# grid_search = GridSearchCV(classifier, params, n_jobs=-1, verbose=1, scoring='roc_auc')\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test[\"Word\"] = test[\"Word\"].apply(lambda x: x.lower())\n",
    "test = add_features(test).drop([\"Word\"], axis=1)\n",
    "pred = regr.predict(test)\n",
    "# test\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pred).to_csv(\"submission_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3]).to_csv(\"test.csv\", index_label=[\"Id\",\"Prediction\"], header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,Prediction,0\r\n",
      "0,1\r\n",
      "1,2\r\n",
      "2,3\r\n"
     ]
    }
   ],
   "source": [
    "! cat test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
