{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-36a4076364a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import re\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "df_majority = data[data.Label==0]\n",
    "df_minority = data[data.Label==1]\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=df_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "data = pd.concat([df_majority, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vowel = u'уеёыаоэяию'\n",
    "consonant = u'йцкнгшщзхъфвпрлджчсмтьб'\n",
    "ru_letter = vowel + consonant + \"-’'\"\n",
    "last_letter = list(set(data[\"Word\"].apply(lambda x: x[-1]).get_values()))\n",
    "last_2_letter = list(set(data[\"Word\"].apply(lambda x: x[-2:]).get_values()))\n",
    "\n",
    "def count_vow(word):\n",
    "    vow = 0\n",
    "    for i in word:\n",
    "        if i in vowel:\n",
    "            vow += 1\n",
    "    return vow\n",
    "\n",
    "def count_con(word):\n",
    "    cons = 0\n",
    "    for i in word:\n",
    "        if i in consonant:\n",
    "            cons += 1\n",
    "    return cons\n",
    "\n",
    "def is_only_ru(word):\n",
    "    return len(set(word) - set(ru_letter)) == 0\n",
    "\n",
    "\n",
    "def last_letters(data):\n",
    "    for letter in  last_letter:\n",
    "        data[\"last1\" + letter] = data[\"Word\"].apply(lambda x: x[-1] == letter)\n",
    "    for letter in last_2_letter:\n",
    "        data[\"last2\" + letter] = data[\"Word\"].apply(lambda x: x[-2:] == letter)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    data[\"Word\"] = data[\"Word\"].apply(lambda x: x.lower())#.decode('utf-8').lower())\n",
    "    data[\"len\"] = data[\"Word\"].apply(lambda x: len(x))\n",
    "    data[\"vow_count\"] = data[\"Word\"].apply(lambda x: count_vow(x))\n",
    "    data[\"cons_count\"] = data[\"Word\"].apply(lambda x: count_con(x))\n",
    "    data[\"last_vow\"] = data[\"Word\"].apply(lambda x: x[-1] in vowel)\n",
    "    data[\"prelast_vow\"] = data[\"Word\"].apply(lambda x: len(x) > 2 and x[-2] in vowel)\n",
    "    data[\"only_ru\"] = data[\"Word\"].apply(lambda x: is_only_ru(x))\n",
    "#     last_2_letter = pd.get_dummies(data[\"Word\"].apply(lambda x: x[-2:]), prefix=\"last2\")\n",
    "#     last_1_letter = pd.get_dummies(data[\"Word\"].apply(lambda x: x[-1:]), prefix=\"last1\")\n",
    "#     data = pd.concat([data,last_2_letter,last_1_letter], axis=1)\n",
    "    data = last_letters(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706467681283\n"
     ]
    }
   ],
   "source": [
    "data = add_features(data)\n",
    "X = data.drop([\"Label\", \"Word\"], axis=1)\n",
    "y = data[\"Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "regr = SGDClassifier()\n",
    "regr = RandomForestClassifier(max_depth=3)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "print(roc_auc_score(y_pred, y_test.get_values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'n_estimators': 60}\n",
      "0.7926004336066128\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "params = {\n",
    "    'max_depth': [22, 25, 30, 40,50 ],\n",
    "    'n_estimators' : [40, 50, 60]\n",
    "}\n",
    "grid_search = GridSearchCV(classifier, params, n_jobs=-1, verbose=1, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = add_features(pd.read_csv(\"test.csv\")).drop([\"Word\"], axis=1)\n",
    "pred = regr.predict(test)\n",
    "# test\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pred).to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
