{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install joblib six\n",
    "# !pip install 'gym[all]'\n",
    "# !pip install pyglet==1.2.4\n",
    "# !pip install pyvirtualdisplay\n",
    "# !pip install --no-cache-dir -I pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.animation as animation \n",
    "from IPython.core.debugger import set_trace\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "    \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    print(reward_threshold)\n",
    "    states_batch = np.array(states_batch)\n",
    "    actions_batch = np.array(actions_batch)\n",
    "    \n",
    "    elite_mask = rewards_batch > reward_threshold\n",
    "    elite_states  = states_batch[elite_mask]\n",
    "    elite_actions = actions_batch[elite_mask]\n",
    "#     set_trace()\n",
    "\n",
    "#     if elite_states != []:\n",
    "    elite_states = np.concatenate(elite_states)\n",
    "    elite_actions = np.concatenate(elite_actions)\n",
    "    \n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch,log):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch,percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(rewards_batch,range=[min(rewards_batch), max(rewards_batch)]);\n",
    "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# plt.imshow(env.render(\"rgb_array\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "#                       activation='tanh',\n",
    "#                       warm_start=True, #keep progress between .fit(...) calls\n",
    "#                       max_iter=1, #make only 1 iteration on each .fit(...)\n",
    "#                      )\n",
    "\n",
    "# #initialize agent to the dimension of state an amount of actions\n",
    "# agent.fit([env.reset()]*n_actions, range(n_actions));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(SineNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2, n_hidden_neurons)\n",
    "        self.act1 = torch.nn.Tanh()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 3)\n",
    "        self.sm = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "    \n",
    "def loss(pred, targ):\n",
    "    res = (pred - targ) ** 2\n",
    "    return res.mean()\n",
    "\n",
    "agent = SineNet(20)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1.0e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=10000):\n",
    "    states,actions = [],[]\n",
    "    total_rewards = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        tensor_s = torch.autograd.Variable(torch.Tensor(s))\n",
    "        probs = agent.forward(tensor_s).data.numpy()\n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_rewards += r\n",
    "        s = new_s[:]\n",
    "        if done: break\n",
    "    return states, actions, total_rewards\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x,y, agent):\n",
    "    batch_size = 100\n",
    "    order = np.random.permutation(x.shape[0])\n",
    "\n",
    "    for start_index in range(0, x.size, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_items = order[start_index:start_index+batch_size]\n",
    "        if batch_items.size == 0:\n",
    "            break\n",
    "            \n",
    "        x_batch = torch.FloatTensor(x[batch_items])#.reshape([-1, 1]))\n",
    "        y_batch = torch.FloatTensor(y[batch_items].reshape([-1, 1]))\n",
    "\n",
    "        x_var = torch.autograd.Variable(x_batch)\n",
    "        y_var = torch.autograd.Variable(y_batch)\n",
    "\n",
    "        y_pred = agent.forward(x_var)\n",
    "        loss_val = loss(y_pred, y_var)\n",
    "\n",
    "        loss_val.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.32615387, 0.3552597 , 0.31858644], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.Tensor([env.reset()]*(n_actions+2))\n",
    "# y = torch.Tensor(list(range(n_actions)) + [2,2])\n",
    "x = torch.Tensor([env.reset()]*n_actions)\n",
    "y = torch.Tensor(list(range(n_actions)))\n",
    "for _ in range(2):\n",
    "    for i in range(len(x)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_train = torch.autograd.Variable(x[i])\n",
    "        y_pred = agent.forward(x_train)\n",
    "        loss_val = loss(y_pred, y[i])\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "agent.forward(torch.autograd.Variable(torch.Tensor([-0.53422085, 0.]))).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_sessions = 300\n",
    "percentile = 90\n",
    "log = []\n",
    "epochs = 100\n",
    "value_to_stop = -200\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = dt.now()\n",
    "    sessions = Parallel(n_jobs=4)(delayed(generate_session)() for _ in range(n_sessions))\n",
    "#     sessions = [generate_session() for _ in range(n_sessions)]\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "    elite_states, elite_actions = select_elites(states_batch,\n",
    "                                                actions_batch,\n",
    "                                                rewards_batch,\n",
    "                                                percentile=percentile)\n",
    "\n",
    "    fit(elite_states, elite_actions, agent)\n",
    "    \n",
    "    show_progress(rewards_batch, log)\n",
    "    print(\"iteration time: {t} s\".format(t=(dt.now() - t).seconds))\n",
    "    if np.mean(rewards_batch) > value_to_stop:\n",
    "        print(\"You win\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
